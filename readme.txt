-------------3/23/22 train ------------

dataset: https://github.com/experiencor/raccoon_dataset
	ln to this folder
	raccoon_dataset -> /media/student/data5/raccoon_dataset/

edit config.json:
	"labels":               ["raccoon"]
        "train_image_folder":   "/home/student/Documents/keras-yolo2/raccoon_dataset/images/",
        "train_annot_folder":   "/home/student/Documents/keras-yolo2/raccoon_dataset/annotations/",

python gen_anchors.py -c config.json


----------------------------------------------------
3/21/22:
pyenv shell 2.7.17
   pip install ...
	pip install tensorflow==1.13.1
	pip install keras==2.0.8 opencv-python==4.2.0.32 imageio==2.6.1
	pip install imgaug tqdm
	
export LD_...=/usr/local/cuda/lib64:$LD...

https://github.com/rodrigo2019/keras_yolo2/releases/tag/pre-trained-weights
	full_yolo_backend.h5
https://drive.google.com/drive/folders/10oym4eL2RxJa0gro26vzXK__TtYOP5Ng
	full_yolo_raccoon.h5

python predict.py -c config.json -w ./full_yolo_raccoon.h5 -i images/cat.jpg
Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR

seems to be GPU memory not enough:
watch -n 0.1 nvidia-smi
8GB all used up @newpcamd

hptitan works.
('backend output shape:', (None, 13, 13, 1024))
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 416, 416, 3)   0                                            
____________________________________________________________________________________________________
conv_1 (Conv2D)                  (None, 416, 416, 32)  864         input_3[0][0]                    
____________________________________________________________________________________________________
norm_1 (BatchNormalization)      (None, 416, 416, 32)  128         conv_1[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 416, 416, 32)  0           norm_1[0][0]                     
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 208, 208, 32)  0           leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
conv_2 (Conv2D)                  (None, 208, 208, 64)  18432       max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
norm_2 (BatchNormalization)      (None, 208, 208, 64)  256         conv_2[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 208, 208, 64)  0           norm_2[0][0]                     
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 104, 104, 64)  0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
conv_3 (Conv2D)                  (None, 104, 104, 128) 73728       max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
norm_3 (BatchNormalization)      (None, 104, 104, 128) 512         conv_3[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 104, 104, 128) 0           norm_3[0][0]                     
____________________________________________________________________________________________________
conv_4 (Conv2D)                  (None, 104, 104, 64)  8192        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
norm_4 (BatchNormalization)      (None, 104, 104, 64)  256         conv_4[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 104, 104, 64)  0           norm_4[0][0]                     
____________________________________________________________________________________________________
conv_5 (Conv2D)                  (None, 104, 104, 128) 73728       leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
norm_5 (BatchNormalization)      (None, 104, 104, 128) 512         conv_5[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 104, 104, 128) 0           norm_5[0][0]                     
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 52, 52, 128)   0           leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
conv_6 (Conv2D)                  (None, 52, 52, 256)   294912      max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
norm_6 (BatchNormalization)      (None, 52, 52, 256)   1024        conv_6[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 52, 52, 256)   0           norm_6[0][0]                     
____________________________________________________________________________________________________
conv_7 (Conv2D)                  (None, 52, 52, 128)   32768       leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
norm_7 (BatchNormalization)      (None, 52, 52, 128)   512         conv_7[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 52, 52, 128)   0           norm_7[0][0]                     
____________________________________________________________________________________________________
conv_8 (Conv2D)                  (None, 52, 52, 256)   294912      leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
norm_8 (BatchNormalization)      (None, 52, 52, 256)   1024        conv_8[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 52, 52, 256)   0           norm_8[0][0]                     
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 26, 26, 256)   0           leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
conv_9 (Conv2D)                  (None, 26, 26, 512)   1179648     max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
norm_9 (BatchNormalization)      (None, 26, 26, 512)   2048        conv_9[0][0]                     
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 26, 26, 512)   0           norm_9[0][0]                     
____________________________________________________________________________________________________
conv_10 (Conv2D)                 (None, 26, 26, 256)   131072      leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
norm_10 (BatchNormalization)     (None, 26, 26, 256)   1024        conv_10[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 26, 26, 256)   0           norm_10[0][0]                    
____________________________________________________________________________________________________
conv_11 (Conv2D)                 (None, 26, 26, 512)   1179648     leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
norm_11 (BatchNormalization)     (None, 26, 26, 512)   2048        conv_11[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 26, 26, 512)   0           norm_11[0][0]                    
____________________________________________________________________________________________________
conv_12 (Conv2D)                 (None, 26, 26, 256)   131072      leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
norm_12 (BatchNormalization)     (None, 26, 26, 256)   1024        conv_12[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 26, 26, 256)   0           norm_12[0][0]                    
____________________________________________________________________________________________________
conv_13 (Conv2D)                 (None, 26, 26, 512)   1179648     leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
norm_13 (BatchNormalization)     (None, 26, 26, 512)   2048        conv_13[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 26, 26, 512)   0           norm_13[0][0]                    
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 13, 13, 512)   0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
conv_14 (Conv2D)                 (None, 13, 13, 1024)  4718592     max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
norm_14 (BatchNormalization)     (None, 13, 13, 1024)  4096        conv_14[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 13, 13, 1024)  0           norm_14[0][0]                    
____________________________________________________________________________________________________
conv_15 (Conv2D)                 (None, 13, 13, 512)   524288      leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
norm_15 (BatchNormalization)     (None, 13, 13, 512)   2048        conv_15[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 13, 13, 512)   0           norm_15[0][0]                    
____________________________________________________________________________________________________
conv_16 (Conv2D)                 (None, 13, 13, 1024)  4718592     leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
norm_16 (BatchNormalization)     (None, 13, 13, 1024)  4096        conv_16[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 13, 13, 1024)  0           norm_16[0][0]                    
____________________________________________________________________________________________________
conv_17 (Conv2D)                 (None, 13, 13, 512)   524288      leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
norm_17 (BatchNormalization)     (None, 13, 13, 512)   2048        conv_17[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 13, 13, 512)   0           norm_17[0][0]                    
____________________________________________________________________________________________________
conv_18 (Conv2D)                 (None, 13, 13, 1024)  4718592     leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
norm_18 (BatchNormalization)     (None, 13, 13, 1024)  4096        conv_18[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 13, 13, 1024)  0           norm_18[0][0]                    
____________________________________________________________________________________________________
conv_19 (Conv2D)                 (None, 13, 13, 1024)  9437184     leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
norm_19 (BatchNormalization)     (None, 13, 13, 1024)  4096        conv_19[0][0]                    
____________________________________________________________________________________________________
conv_21 (Conv2D)                 (None, 26, 26, 64)    32768       leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 13, 13, 1024)  0           norm_19[0][0]                    
____________________________________________________________________________________________________
norm_21 (BatchNormalization)     (None, 26, 26, 64)    256         conv_21[0][0]                    
____________________________________________________________________________________________________
conv_20 (Conv2D)                 (None, 13, 13, 1024)  9437184     leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 26, 26, 64)    0           norm_21[0][0]                    
____________________________________________________________________________________________________
norm_20 (BatchNormalization)     (None, 13, 13, 1024)  4096        conv_20[0][0]                    
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 13, 13, 256)   0           leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 13, 13, 1024)  0           norm_20[0][0]                    
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 13, 13, 1280)  0           lambda_1[0][0]                   
                                                                   leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
conv_22 (Conv2D)                 (None, 13, 13, 1024)  11796480    concatenate_1[0][0]              
____________________________________________________________________________________________________
norm_22 (BatchNormalization)     (None, 13, 13, 1024)  4096        conv_22[0][0]                    
____________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)       (None, 13, 13, 1024)  0           norm_22[0][0]                    
====================================================================================================

front end
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 416, 416, 3)   0                                            
____________________________________________________________________________________________________
model_1 (Model)                  (None, 13, 13, 1024)  50547936    input_1[0][0]                    
____________________________________________________________________________________________________
DetectionLayer (Conv2D)          (None, 13, 13, 30)    30750       model_1[1][0]                    
____________________________________________________________________________________________________
reshape_1 (Reshape)              (None, 13, 13, 5, 6)  0           DetectionLayer[0][0]             
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1, 1, 1, 10, 4 0                                            
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, 13, 13, 5, 6)  0           reshape_1[0][0]                  
                                                                   input_2[0][0]                    
====================================================================================================
Total params: 50,578,686
Trainable params: 50,558,014
Non-trainable params: 20,672


--------------3/22/22 about yolo2 impl --------------
this impl is same as the original impl. 

	https://github.com/zzxvictor/YOLO_Explained
the explaination of jonathan-hui is not correct in the last few layers of yolo2
the original paper quota " removing the last conv layer (of darknet19) and adding three 3x3 conv each with 1024 filters... 
	also add a passthrough layer
	from ... to the 2nd last conv layer". 
	the paper did not say the passthrough is a conv, but the code suggest that.
	the number of filter for  the passthrough layer is not specified in paper,
		but the code suggest it is 64, as done in this impl.
	the paper also did not explicitly say that the last layer is 125x1x1
		so compared to darknet19, yolo2 contain a total of 23 conv layers.
	the last conv layer of darknet19 is 1000x1x1, here 1000 is for 1000 classes when training with imagenet 1k.

passthrough layer:
	jonathan-hui says the passthrough take the 26x26x512 output and reshape to 13x13x2048. this is backed by the original paper
	at page 3 (fine-grained features)
	in both this impl and the original darknet repo, there is only a 1x1x64 filter applied to the output of layer 13 leaky_re_lu_13
	(before maxpool )
	that would generate output 26x26x64, then it apply space_to_depth rearrange, the result output tensor should be 13x13x256,
	this is still not resolved myth. lambda_1	
	a possible explaination is that the original paper did something on the passthrough layer and did not use 64x1x1 conv, 
	it actually is a reorg passthrough. the code in this impl and some other code all did wrong on this part.
	https://github.com/zzxvictor/YOLO_Explained


from https://github.com/juwangvsu/darknet.git
	cfg/yolo2.cfg, yolo2-voc.cfg --- yolo2.cfg train for coco dataset 80 classes 1k, voc train for voc dataset 20 classes
	cfg/darknet19.cfg
darknet19 ---- last layer softmax classifier of 1000 classes. trained with imagenet 1k dataset
yolo2 ------- darknet19's first 18 layers. the last layer of darknet19 is replaced by 3 conv: 1024x3x3, plus conv: 64x3x3 for a pass through
		route, plus the last layer conv 125x1x1, 125 = 5x(5+20) for 20 classes detection. for 80 classes (coco), the last layer
		is 425x1x1 conv layer.
		the output of the 125x1x1 layer is a tensor of 13x13x125, 5 boxes per grid cell, each box contain coord, and 20 classes probability.

 
